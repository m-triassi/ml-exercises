{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9 - Convolutional Neural Networks\n",
    "\n",
    "In this lab you'll use [PyTorch](https://pytorch.org/) to train a *convolutional neural network* (CNN, or ConvNet) on synthetic and real data, and to inspect both the filters and the feature maps of your model to ensure you understand the inner workings (intermediate representations) that are computed during the feed-forward computation of a ConvNet.\n",
    "\n",
    "If you really want to understand convolutions in detail, you can also download the *convolution_layer_demo.zip* file in the course Moodle page and run the Jupyter notebook inside. (All the code in that notebook is done for you.)\n",
    "\n",
    "**Run the code cell below** to import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import copy\n",
    "np.set_printoptions(precision=3, suppress=True)  # Print array values as 0.0023 instead of 2.352e-3\n",
    "torch.set_printoptions(precision=3, sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the code cell below** to define a plotting function that is useful for visualizing the weights of neural networks and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix_grid(V, cmap='bwr'):\n",
    "    \"\"\"\n",
    "    Given an array V containing stacked matrices, plots them in a grid layout.\n",
    "    V should have shape (K,M,N) where V[k] is a matrix of shape (M,N).\n",
    "    The default cmap is \"bwr\" (blue-white-red) but can also be \"gray\".\n",
    "    \"\"\"\n",
    "    if isinstance(V, torch.Tensor):\n",
    "        V = V.detach().numpy()\n",
    "    assert V.ndim == 3, \"Expected V to have 3 dimensions, not %d\" % V.ndim\n",
    "    k, m, n = V.shape\n",
    "    ncol = 8                                     # At most 8 columns\n",
    "    nrow = min(4, (k + ncol - 1) // ncol)        # At most 4 rows\n",
    "    V = V[:nrow*ncol]                            # Focus on just the matrices we'll actually plot\n",
    "    figsize = (2*ncol, max(1, 2*nrow*(m/n)))     # Guess a good figure shape based on ncol, nrow\n",
    "    fig, axes = plt.subplots(nrow, ncol, sharex=True, sharey=True, figsize=figsize)\n",
    "    vmax = np.percentile(np.abs(V), [99.9])      # Show the main range of values, between 0.1%-99.9%\n",
    "    for v, ax in zip(V, axes.flat):\n",
    "        img = ax.matshow(v, vmin=-vmax, vmax=vmax, cmap=plt.get_cmap(cmap))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    for ax in axes.flat[len(V):]:\n",
    "        ax.set_axis_off()\n",
    "    fig.colorbar(img, cax=fig.add_axes([0.92, 0.25, 0.01, .5]))   # Add a colorbar on the right    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black\"></div>\n",
    "\n",
    "# 1. Convolutional Neural Networks on Synthetic Data\n",
    "\n",
    "Exercise 1.1&ndash;1.4 ask you to generate a synthetic data set and to inspect how a 1D convolutional neural network learns \"pattern detectors.\" Using a 1D data set makes it easy to plot all the training features, targets, and predictions, seeing them all at once.\n",
    "\n",
    "**Run the code cell below** to define a function that will be useful for plotting data and predictions over the synthetic training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_named_tensors(tensor_dict):\n",
    "    \"\"\"\n",
    "    Given a dict of {name: tensor} pairs, plots the tensors side-by-side in a common\n",
    "    color scale. The name of each tensor is shown above its plot.\n",
    "    \"\"\"\n",
    "    n = len(tensor_dict)\n",
    "    vmax = max(v.abs().max() for v in tensor_dict.values())\n",
    "    figsize = (2*n, 6)\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize,  constrained_layout=True, squeeze=True)\n",
    "    axes = axes.flat if isinstance(axes, np.ndarray) else (axes,)\n",
    "    for (name, v), ax in zip(tensor_dict.items(), axes):\n",
    "        v = torch.squeeze(v.detach())   # Automatically convert (N,1,D) to (N,D)\n",
    "        if v.ndim == 1:\n",
    "            v = v.view(-1, 1)  # Automatically convert (N,) to (N,1)\n",
    "        assert v.ndim == 2, \"couldn't turn tensors[%d] with shape %s into 2D\" % (i, v.shape)\n",
    "        img = ax.matshow(v, vmin=-vmax, vmax=vmax, cmap=plt.get_cmap('bwr'))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(name)\n",
    "    fig.colorbar(img, cax=fig.add_axes([0.985, 0.25, 0.03, .5]))   # Add a colorbar on the right    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1 &ndash; Build a synthetic 1D data set\n",
    "\n",
    "Here you are asked to write code that uses random numbers to build a synthetic training set $(\\mathbf{X},\\mathbf{y})$ with $N=75$ rows where the $i^\\text{th}$ row $\\mathbf{x}_i \\in \\mathbb{R}^{C \\times W}$ represents a 1-dimensional \"image\" with $C=1$ color channels (grayscale) and width $W=20$. having binary class label $y_i \\in \\{0, 1\\}$. (Note that $W$ here is just a number and that the use of symbol $W$ is a convension for specifying ConvNet tensor shapes; try not to confuse this with using symbol $\\mathbf{W}$ to represent a matrix of parameters.)\n",
    "\n",
    "Since we will use this data set to train a PyTorch model, you will build PyTorch tensors rather than Numpy arrays. However, you will still use the Numpy random number functions because they're simpler.\n",
    "\n",
    "The idea is to build a classification data set, where each input is a 1-dimensional image, and the image is classified as positive if and only if it contains a specific sequence of values $\\ldots,0,1,0,\\ldots$ somewhere in the image. **The obvious way to classify those images as positive is to learn a little filter that 'activated' by the local pattern $[0,1,0]$ the sequence.**\n",
    "\n",
    "Specifically, initialize each synthetic training case from $i=0,\\ldots,N-1$ will be initialially all zeros and then constructed exactly as follows:\n",
    "1. Sample a random integer $\\text{size} \\in \\{1,2\\}$, _i.e._ the size of the subrange of pixels that will be assigned.\n",
    "2. Sample a random integer $\\text{start} \\in \\{0,\\ldots,W-\\text{size}-1\\}$, _i.e._ the offset from the start of the image\n",
    "3. Sample a random choice $\\text{value} \\in \\{-1, -\\frac{1}{2}, \\frac{1}{2}, 1\\}$, _i.e._ the value to fill for the subrange\n",
    "4. Assign features $X[i,0, \\text{start}:\\text{start}+\\text{size}] = \\text{value}$, _i.e._ set either a 1- or 2-pixel subrange to be the 'value'.\n",
    "5. Assign target $y[i]$ to be 1 if both $\\text{size}=1$ and $\\text{value} = +1$, _i.e._ if $X[i,0,:]$ has a single \"+1\" pixel somewhere in it.\n",
    "\n",
    "The goal here is to get you to practice using the random number functions (e.g. *randint*, *choice*) and to build a synthetic data set that will be useful for \"inspecting\" a simple 1D ConvNet. Note that the *randint(a,b)* returns an integer strictly less than *b*.\n",
    "\n",
    "<span style=\"color:red\">If PyTorch complains about not being able to convert *numpy.int32* or *numpy.bool_* to *torch.FloatTensor* you may need to explicitly convert the value to float using *float(value)*.</span>\n",
    "\n",
    "The\n",
    "**[torch.nn.Conv1d](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv1d)** module expects its input features to be stored as a tensor $\\mathbf{X} \\in \\mathbb{R}^{N \\times C \\times W}$, so that is the reason behind the strange shape of our feature tensor.\n",
    "\n",
    "**Complete the starter code below** by adding a for loop that implements the above instructions for building the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)  # For reproducibility\n",
    "\n",
    "N = 75  # N = number of training cases\n",
    "C = 1   # C = number of channels (just 1 in our case)\n",
    "W = 20  # W = width of the 1-dimensional input image\n",
    "\n",
    "# We create feature tensor X in \"(N,C,W) format\" (the shape) so that it can\n",
    "# be used directly as input to PyTorch's Conv1D module.\n",
    "X = torch.zeros((N, C, W))   # Start with zeros. You need to assign some of these feature values!\n",
    "y = torch.zeros((N, 1))      # Start with zeros. You need to assign some of these target values!\n",
    "\n",
    "for i in range(N):\n",
    "    size = np.random.choice([1,2])\n",
    "    start = np.random.choice(W - size - 1)\n",
    "    value = np.random.choice([-1, -0.5, 0.5, 1])\n",
    "    X[i, 0, start : start + size] = value\n",
    "    y[i] = (0, 1)[1 in X[i, 0, :]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below. It will plot the entire training set as a heatmap, where blue indicates negative values and red indicates positive values. If you initialized your training set correctly (and kept `np.random.seed(0)` above) you should see exactly this training set:\n",
    "![image](img/synthetic_training_set.png)\n",
    "Here we have $N=75$ rows and $W=20$ columns shown for the $\\mathbf{X}$ tensor. Notice that the red dots in the $\\mathbf{y}$ vector correspond to the $y_i=1$ for which there is exactly a 1-pixel red dot (a $+1$) in the corresponding input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 1, 20])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAG4CAYAAAAjc64XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAaJUlEQVR4nO3df4wcZ33H8c/nHJyopS0OThPbSeMgXBVQ2tC1QyXaQkMghj/iFEIwNKpTJa1SkbYqIsJRJMc2UAX6R6S2Ec0VDOaHMGkoylUYuflBSiUIvTthBRwaYkxC/AOC84P+EUjq3Ld/7Bwan2fnmb2dvd2dfb+kkXeemd15dtf30TPz3ZlxRAgA0NnEoDsAAMOOoASABIISABIISgBIICgBIIGgBIAEghIAEsYyKG2/1PZjtv8k1/Yrtn9o+8pB9g3A8PG4/uDc9mWSPivp1RHxE9sfk3R2RLx9wF0DMGTGNiglyfanJJ0u6Q5JX5T0moj40UA7hZFh+0ZJvxcR78i1/YOkiIi/GVzPULdxD8oVkh6W9BJJN0bEJwfcJYwQ26skHZS0JiKetX2apKOS3hoRs4PtHeo0lsco50XEM5IOSPolSf824O5gxETEMUlfk/TOrGmjpOOEZPOMdVDavlrSWkn3SvrIYHuDEbVb0tXZ46slfWaAfUGfjO2ut+1fV3s0eZWk/8keb4qI/xpoxzBSbJ8h6ZikP5D0oNrFwR8Otleo2zgH5Z2SfhoRf57NXyfp/ZJ+JyKeH2jnMFJs/4uk16m9233JoPuD+o3lrrftKyT9vqQb59si4uNqH4jfNqBuYXTtlnSh2O1urLEdUQJ1sf0bah++OSci/nfQ/UH9xnJECdTF9oSk90naQ0g212mD7gAwqmz/sqQfS3pc7Z8GoaHY9QaABHa9ASCBoASAhK6OUa5cuTLWrl3bp66Mj6NHT21bvbr+7Tz22GM6fvy4639lzCv6m+Bzb56ugnLt2rWamZ7uV1/Gxo6dp/4N3bKt/mPF6zdsqP01cbKivwk+9+Zh1xsAEghKAEhY2t9RTnTI5bm5Je3GoPVjNxsDMjvb+f81GoNvGAASCEoASCAoASCBoASABIISABKWturdp+p20Q+4O6HijFq1WtLCkzD4wXnjMKIEgASCEgASCEoASCAoASBhOG4F0c0pYAUFIQo0GBhOYRwLfMMAkEBQAkACQQkACQQlACQQlACQMBxV71G6cO/27d21o9k4hXEsMKIEgASCEgASCEoASCAoASBhOIo53ejxdMeeUbRBHqcwjgW+YQBIICgBIIGgBIAEghIAEghKAEgYvaCcm6s+DZgnXHnCiGq1Tv1/12oNuleo2egFJQAsMYISABIISgBIICgBIGFoT2HcsbN6gWNY78IYc8PZLwDdYUQJAAkEJQAkEJQAkEBQAkDC0BZz6ijQVC0IDWsxCMBwYEQJAAkEJQAkEJQAkEBQAkACQQkACUNb9a7DSFWzO93Jbwiuq4kS3IVxLPANA0ACQQkACQQlACQQlACQ0OhiztDi4D8wUviLBYAEghIAEghKAEggKAEggaAEgIQlDUpPuHAaO3Nz1ScMt1br1O+s1Rp0r1AzRpQAkEBQAkACQQkACQQlACQs6SmMMTdC14esQzenKlK4GU1cj3Is8A0DQAJBCQAJBCUAJBCUAJBAUAJAQs9V7x07i09BHKk7IPYLlezma7Wk6emT2zZsGExf0DeMKAEggaAEgASCEgASCEoASOiumFNwutYtndbdRiEDY4BTGMcC3zAAJBCUAJBAUAJAAkEJAAndFXOKzkLoxvbt3bUDw44zc8YCI0oASCAoASCBoASABIISABIISgBDx/Yu20/a/k6H5bb9D7YP2n7I9u/mlm2x/Wg2bamjP327C6Mniq5TuaNw3djOtSsBnORTkv5J0qc7LH+rpHXZ9DpJH5P0Ottnqn1m9XpJIWnW9lREPNNLZxhRAhg6EfE1SU+XrLJJ0qej7UFJL7O9StJlku6JiKezcLxH0sZe+0NQAhhFayQ9kZs/nLV1au9J33a9AYynjXYcT6wzKx2Q9PNc02RETPaxWz0hKAHU6rikmcSl5zw39/OIWN/DZo5IOi83f27WdkTSGxe0P9DDdiT1MShjrnqBpugGZdycDBhhqWt09n7jvSlJN9jeo3Yx56cRccz2Pkl/Z3tFtt5bJN3U68YYUQKol93zxYxtf17tkeFK24fVrmS/RJIi4p8l7ZX0NkkHJT0n6c+yZU/b/qCk+RPwd0ZEWVGoEoISQP16DMqIeHdieUh6b4dluyTt6qkDCxCUAOplS6c1K1qa9W4ADIeG3UeIoARQrxqOUQ6boQjKbircxadGFuum8g6gRgQlAJRgRAkAFVDMAYASjCgBoAKCcrDGrkBT9B+u99O/UJfZ2caFQs8YUQJABQQlAJTgzBwAqIARJQCU4BglAFRAUI647durtQ0LKtzDrdWSpqdPbtuwYTB9GSYEJQCUYNcbABKoegNABQ0bUTbr3QAYDhMT5VMFtjfafsT2QdtbC5bfZnt/Nn3P9rO5ZS/mlk31+nZ6HlEW3UFRGuK7KA5z4QZognpuLrZM0u2S3izpsKRp21MR8fD8OhHxt7n1/0rSa3Mv8bOIuKinTuQwogRQv95HlBdLOhgRhyLiBUl7JG0qWf/dkj5fQ88LEZQA6jVfzCmb0tZIeiI3fzhrK9icz5d0gaT7c81n2J6x/aDtKxb5Tn6BYg6A+qVHjSttz+TmJyNicpFb2yzproh4Mdd2fkQcsf0KSffb/nZEfH+Rr09QAqhZtWOUxyNifcnyI5LOy82fm7UV2awF9/iOiCPZv4dsP6D28cvBBeXQFm066FR8KjJq7w0YGr3/PGha0jrbF6gdkJslvWfhSrZ/S9IKSd/Ita2Q9FxEPG97paTXS/poL51hRAmgXjVUvSPihO0bJO2TtEzSrog4YHunpJmImP/Jz2ZJeyIiP6p5laQ7bM+pXYe5NV8tXwyCEkD9avjBeUTslbR3Qdu2BfPbC573dUkX9tyBHIISQL04hREAKmjYKYwEJYB6cfWgIdDNF1BwLUcq2agVd2Es1rDPZPSCEsBwY0QJABVQzAGAEowoAaACgnLARulmW91c+5LrZI4mbi52KkaUAFABQQkAJTgzBwAqYEQJACU4RgkAFYx1UB49Wr06SxWXzwDja6yDEgBS2PUGgASq3gBQQcNGlM16NwCGw8RE+VSB7Y22H7F90PbWguXX2P6J7f3ZdF1u2Rbbj2bTll7fTncjytWrx75AwV0cgYQajlHaXibpdklvlnRY0rTtqYKbhH0hIm5Y8NwzJd0iab2kkDSbPfeZxfaHESWA+vU+orxY0sGIOBQRL0jaI2lTxa1fJumeiHg6C8d7JG1c1PvIEJQA6jVfzCmb0tZIeiI3fzhrW+gdth+yfZft87p8bmUEJYD6pUeUK23P5Ka/WMRW/l3S2oj4bbVHjbvrfAt5VL0B1KvaMcrjEbG+ZPkRSefl5s/N2n4hIp7KzX5c0kdzz33jguc+kOpQGUaUAOrX+zHKaUnrbF9ge7mkzZKm8ivYXpWbvVzSd7PH+yS9xfYK2yskvSVrWzRGlCU8cWqFO+aoZAOlaqh6R8QJ2zeoHXDLJO2KiAO2d0qaiYgpSX9t+3JJJyQ9Lema7LlP2/6g2mErSTsj4ule+kNQAqhfDT84j4i9kvYuaNuWe3yTpJs6PHeXpF09dyJDUAKoF6cwAkAFDTuFkaAEUC+uHtQnRR/qENxtkcINkmZnGxcKtWjYZzIcQQmgWQhKAChBMQcAEjhGCQAVNCwol/TdeMLFk+KUCRgJrVa78JifWq1B92rwarhw7zBhRAmgXux6A0ACxRwAqIARJQCUYNcbACogKBdvyU8JLLpj5JjfRRJYEgQlAJRg1xsAEqh6A0AFDRtRNuvdABgONZyZY3uj7UdsH7S9tWD5+2w/nN3X+z7b5+eWvWh7fzZNLXxut5Z2RNlNIaWOoguFG2Dp1XCM0vYySbdLerOkw5KmbU9FxMO51b4laX1EPGf7L9W+Xe27smU/i4iLeupEDiNKAPXrfUR5saSDEXEoIl6QtEfSpvwKEfHViHgum31Q7ft39wVBCaBe88WcsiltjaQncvOHs7ZOrpX0ldz8GbZnbD9o+4qu38MCFHMA1C89alxpeyY3PxkRk4vZlO2rJa2X9IZc8/kRccT2KyTdb/vbEfH9xby+RFACqFu1Y5THI2J9yfIjks7LzZ+btS3YlC+VdLOkN0TE8/PtEXEk+/eQ7QckvVbSooOSXW8A9ev9GOW0pHW2L7C9XNJmSSdVr22/VtIdki6PiCdz7Stsn549Xinp9ZLyRaCuDW/Vu8F27HTldW/ZxkWMMWJqqHpHxAnbN0jaJ2mZpF0RccD2TkkzETEl6e8lvVTSv9qWpB9GxOWSXiXpDttzag8Gb11QLe8au94A6lfDD84jYq+kvQvatuUeX9rheV+XdGHPHcghKAHUi1MYAaCChp3CSFACqBdXD0Idigo0nQo8Re0UeDD0CEoASCAoAaAExRwASOAYJQBUQFCOuG6+wLm5/vVjAQo0I2p2tnGhUIuGfSbjF5QA+otdbwBIoJgDABUwogSABIISAEpwjHIJdbp2Za/XtFzCSjbGQKslTU+f3LZhw2D6MkwISgAowYgSABKoegNABQ0bUTbr3QAYDr3fXEy2N9p+xPZB21sLlp9u+wvZ8m/aXptbdlPW/ojty3p9O8M7ouRGZG1FnwOfDYZZDccobS+TdLukN0s6LGna9tSCm4RdK+mZiHil7c2SPiLpXbZfrfZdG18jabWke23/ZkS8uNj+MKIEUL/eR5QXSzoYEYci4gVJeyRtWrDOJkm7s8d3SXqT27dj3CRpT0Q8HxE/kHQwe73Fv51engwAp5gv5pRNaWskPZGbP5y1Fa4TESck/VTSyys+tyvDu+sNYGSFkveuX2l7Jjc/GRGTfexSTwhKALWrcF7H8YhYX7L8iKTzcvPnZm1F6xy2fZqkX5P0VMXndoVdbwC1imgHZdlUwbSkdbYvsL1c7eLM1IJ1piRtyR5fKen+iIisfXNWFb9A0jpJ/93Le+rbiNITpw69Y46L03aNCjdGUK9nCkfECds3SNonaZmkXRFxwPZOSTMRMSXpE5I+Y/ugpKfVDlNl690p6WFJJyS9t5eKt8SuN4CazY8oe3+d2Ctp74K2bbnHP5f0zg7P/bCkD/feizaCEkDtTpwYdA/qRVACqFVdI8phQlACqB1BWVFh4aabwgRFDGAkMaIEgAoISgAoEUExBwCSGFECQAmOUQJABQRlLxpcyeaUTaCNESUAVEAxBwBKMKIEgAoISgAowYiyCbq5O1wX3zaFmzE1O9u4e1jXgaAEgASCEgBKcAojACQ08RglB1cA1K6Gm4t1ZPtM2/fYfjT7d0XBOhfZ/obtA7Yfsv2u3LJP2f6B7f3ZdFFqm+M3oiz6lhp8xhD6rNWSpqdPbtuwYTB9GSJ9HlFulXRfRNxqe2s2/4EF6zwn6U8j4lHbqyXN2t4XEc9my2+MiLuqbpARJYBa1XS72jKbJO3OHu+WdMWpfYjvRcSj2eOjkp6UdNZiN0hQAqjVfDGnbOrR2RFxLHv8I0lnl61s+2JJyyV9P9f84WyX/Dbbp6c2OH673gD6rsKocaXtmdz8ZERMzs/YvlfSOQXPuzk/ExFhu+OPmG2vkvQZSVsiYr5XN6kdsMslTaq9276zrLMEJYDaVQjK4xGxvtPCiLi00zLbP7a9KiKOZUH4ZIf1flXSlyXdHBEP5l57fjT6vO1PSnp/qrPsegOo1RIco5yStCV7vEXS3QtXsL1c0pckfXph0SYLV9m22sc3v5PaYN9GlDt2nnp9xk5u2Tbg0/+oegO16nPV+1ZJd9q+VtLjkq6SJNvrJV0fEddlbX8o6eW2r8med01E7Jf0OdtnSbKk/ZKuT22QXW8Ater3D84j4ilJbypon5F0Xfb4s5I+2+H5l3S7TYISQO04hREASjTxFEaCEkDtCMqKCgs0HYsmndoBjBpGlABQAUEJACW4HiUAVMCIEgBKcIwSACogKPuhqBrOaYXASGJECQAVUMwBgBKMKAGgAoISAEowouyRd+4obI+5AV+PEkCtCEoASCAoAaAEpzACQEITj1FyczEAtevnzcVsn2n7HtuPZv+u6LDei7b3Z9NUrv0C29+0fdD2F7IbkZUiKAHUrs93Ydwq6b6IWCfpvmy+yM8i4qJsujzX/hFJt0XEKyU9I+na1AaXdNd73KrbI3UnSqAmS7DrvUnSG7PHuyU9IOkDVZ6Y3aL2EknvyT1/u6SPlT2PESWAWs0Xc8qmHp0dEceyxz+SdHaH9c6wPWP7QdtXZG0vl/RsRMz34rCkNakNUswBULsKI8qVtmdy85MRMTk/Y/teSecUPO/m/ExEhO1Ou2PnR8QR26+QdL/tb0v6abJnBQhKALWrEJTHI2J9p4URcWmnZbZ/bHtVRByzvUrSkx1e40j27yHbD0h6raQvSnqZ7dOyUeW5ko6kOsuuN4BazR+j7GMxZ0rSluzxFkl3L1zB9grbp2ePV0p6vaSHIyIkfVXSlWXPX6jnEaUnigsW41a4KUKBBuOqz8WcWyXdaftaSY9LukqSbK+XdH1EXCfpVZLusD2n9oDw1oh4OHv+ByTtsf0hSd+S9InUBtn1BlCrfle9I+IpSW8qaJ+RdF32+OuSLuzw/EOSLu5mmwQlgNpxCiMAlGjiKYwEJYDaEZQLNLloU1SoavL7BerAiBIAKiAoAaAE16MEgAoYUQJACY5RAkAFBGVVE12cRj6knyoVbiTNznb3f31MDOmf9KIxogRQK4o5AJDAMUoAqICgBICE8Q7Ko0el7dtPbls4P69pnxRQpNWSpqdPbtuwYTB9GRLsegNABQQlAJSg6g0ACU3c9eaXsgBq18+bi9k+0/Y9th/N/l1RsM4f2d6fm34+f29v25+y/YPcsotS2yQoAdSuz3dh3CrpvohYJ+m+bP4kEfHViLgoIi6SdImk5yT9R26VG+eXR8T+1Aa72/VevbpzlXupFJ0u1rRxPkYHpzCeYgl2vTdJemP2eLekB9S+s2InV0r6SkQ8t9gN8g0DqNV8Mads6tHZEXEse/wjSWcn1t8s6fML2j5s+yHbt83f/7sMxRwAtaswolxpeyY3PxkRk/Mztu+VdE7B827Oz0RE2O549Rrbq9S+be2+XPNNagfsckmTao9Gd5Z1lqAEULsKQXk8ItZ3WhgRl3ZaZvvHtldFxLEsCJ8s2c5Vkr4UEf+Xe+350ejztj8p6f2pzrLrDaBW88co+1jMmZK0JXu8RdLdJeu+Wwt2u7NwlW1LukLSd1IbHL0RZT+OEnc6GE+RCFiUPv/p3CrpTtvXSnpc7VGjbK+XdH1EXJfNr5V0nqT/XPD8z9k+S5Il7Zd0fWqDoxeUAIZav6veEfGUpDcVtM9Iui43/5ikNQXrXdLtNglKALXjFEYAKNHEUxgJSgC1IygBoAQjyhGzY6dPabtlW8FvU5v2rWLpcOHeQk37k2p0UAJYelyPEgAqYEQJACU4RgkAFRCUVQ3BdSMLCzdAnbgeZSGCEgBKUMwBgASOUQJABQQlACQQlABQgl1vAKiAoASAElS9AaCCpo0o+aUsgFr1++Zitt9p+4Dtuew+OZ3W22j7EdsHbW/NtV9g+5tZ+xdsL09tk6AEULs+34XxO5LeLulrnVawvUzS7ZLeKunVkt5t+9XZ4o9Iui0iXinpGUnXpjbYv6Dsw6fTZJ5w5QlDpNU69f95qzXoXg1Uv0eUEfHdiHgksdrFkg5GxKGIeEHSHkmbslvUXiLprmy93WrfsrYUxygB1G4IijlrJD2Rmz8s6XWSXi7p2Yg4kWs/5U6NCxGUAGo2u0/yysRKZ9ieyc1PRsTk/IzteyWdU/C8myPi7jp62Q2CEkCtImJjDa9xaY8vcUTSebn5c7O2pyS9zPZp2ahyvr0UxRwATTQtaV1W4V4uabOkqYgISV+VdGW23hZJyRFqs0eU27dXaxsCMce1M4EqbP+xpH+UdJakL9veHxGX2V4t6eMR8baIOGH7Bkn7JC2TtCsiDmQv8QFJe2x/SNK3JH0itc1mByWAxomIL0n6UkH7UUlvy83vlbS3YL1DalfFK2PXGwASCEoASCAoASCBoASAhCUt5nQ6/a5vFd8hrXADGC2MKAEggaAEgASCEgASCEoASFjSYs64nabXzbUjx+2zAUYJI0oASCAoASCBoASABIISABIISgBIWNKq946dxVXgW7Y1s+JLJRtoBkaUAJBAUAJAAkEJAAkEJQAkLGkxp6lFG6lzoapIkz8HoIkYUQJAAkEJAAkEJQAkEJQAkLCkxZxRw/UkAUiMKAEgiaAEgASCEgASCEoASCAoASCBqneJXivZ43b9TaCpGFECQAJBCQAJBCUAJBCUAJDQczGn02l+nNJH0QZoCkaUAJBAUAJAAkEJAAkEJQAkEJQAkNBz1buW6vZEQV7PzfX+ukC/zc4W//9Fo/ANA0ACQQkACQQlACQQlACQMBzXo+yicNPpGo9FOIUQfddqSdPTJ7dt2DCYvqBvGFECQAJBCQAJBCUAJBCUAJBAUAJAwnBUvbswtJXsbk5j4/TM5uAUxrHANwwACQQlACQQlACQQFACQMLIFXOGVo+nYQ5tkQrlOIVxLDCiBIAEghIAEghKAEggKAEgodnFnO3bq7UtMQo3wGhhRAkACQQlACQQlACQQFACQAJBCQAJ41f17tfzh6CaDqA/GFECQAJBCQAJBCUAJBCUAJDQezGnyTfVokCDFG4uNhb4hgEggaAEgASCEgASCEoASCAoASCh96Ccm6s+NYAnXDhhTLVap/4/b7UG3SvUjBElACQQlACQQFACQAJBCQAJS3s9ygZc3zHmuIMiMG4YUQJAAkEJAAkEJQAkEJQAkEBQAkACQQkACQQlACQQlACQQFACQAJBCQAJjqh+Sp7tn0h6vH/dQc3Oj4izBt2JJuvwN8Hn3jBdBSUAjCN2vQEggaAEgASCEgASCEoASCAoASCBoASABIISABIISgBIICgBIOH/Afe+FZiz9WVJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " plot_named_tensors({'X': X, 'y': y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2 &ndash; Define a ConvNet architecture suitable for classifing your synthetic data\n",
    "\n",
    "\n",
    "You are asked to define a 1D convolutional neural network with the following feed-forward architecture built from the [PyTorch modules](https://pytorch.org/docs/stable/nn.html) mentioned. The architecture should transform an input tensor $X \\in \\mathbb{R}^{N \\times C \\times W}$ into a vector of scalar activations $\\mathbf{a} \\in \\mathbb{R}^{N \\times 1}$. \n",
    "\n",
    "![image](img/conv1d_architecture.png)\n",
    "\n",
    "Comments on the architecture:\n",
    "* The *convolutional* layer is depicted as having exactly enough padding (implicitly zero) to ensure that the output feature maps have the same spatial length ($W=20$) as the input vector had. Notice that if you were to increase the filter size by 2, you need to increase the padding by 1 (padding appears on both ends) to keep the output of convolution the same length as before. See the documentation for the Conv1d module.\n",
    "* The *max pooling* layer is being used here to take the maximum value across each of the 3 feature maps shown. Since we want to take the max over the entire spatial extent of the feature map, we use a large *kernel_size*.\n",
    "* The 1-dimensional convolution and pooling operations require *spatial* data in $(N,C,W)$ format (where $W$ is the spatial dimension), so that the operators know how long the spatial dimension is. However the *linear* (fully-connected) layer doesn't know how to deal with spatial data, so the *flatten* operation simply reshapes the tensor from shape $(N,C,W)$ to shape $(N,D)$ where $D=C \\times W$.\n",
    "* Even though we'll use this architecture for binary classification, we do not add an extra *sigmoid* operation directly to the output. This is done for same reasons as for the multi-class PyTorch neural network from Lab 8. After the model is defined, we can still use it to predict binary class probabilities $\\hat{y}_i \\in [0,1]$ by feeding features $X$ through the model and then feeding the resulting activations $\\mathbf{a}$ through a sigmoid so that class predictions are $\\mathbf{\\hat{y}} = \\sigma(\\mathbf{a})$. \n",
    "\n",
    "**Add a few lines of code** to define a PyTorch model with the above architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # Ensure model weights initialized with same random numbers\n",
    "\n",
    "num_filter = 4   # The number of filters to learn\n",
    "filter_size = 5  # The size of each filter\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv1d(1, num_filter, 5, padding=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool1d(W),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(num_filter, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your model architecture** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks OK!\n"
     ]
    }
   ],
   "source": [
    "assert len(model) == 5, \"Should be 5 layers!\"\n",
    "assert isinstance(model[0], torch.nn.Conv1d), \"layer 0 should be Conv1d\"\n",
    "assert model[0].in_channels == C, \"layer 0 should expect C input channel\"\n",
    "assert model[0].out_channels == num_filter, \"layer 0 should expect %d output channels\" % num_filter\n",
    "assert model[0].kernel_size[0] == filter_size, \"layer 0 filter size should be %d\" % filter_size\n",
    "assert model[0].padding[0] == filter_size//2\n",
    "assert isinstance(model[1], torch.nn.ReLU), \"layer 1 should be ReLU\"\n",
    "assert isinstance(model[2], torch.nn.MaxPool1d), \"layer 2 should be MaxPool1d\"\n",
    "assert model[2].kernel_size == W, \"layer 2 should pool over the entire input feature map\"\n",
    "assert isinstance(model[3], torch.nn.Flatten), \"layer 3 should by Flatten\"\n",
    "assert isinstance(model[4], torch.nn.Linear), \"layer 4 should be Linear\"\n",
    "assert model[4].in_features == num_filter, \"layer 4 should have accept %d inputs\" % num_filter\n",
    "assert model[4].out_features == 1, \"layer 4 should have only 1 output\"\n",
    "print(\"Looks OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot your model's initial predictions** (i.e. *before* training) by running the code cell below. Notice that the untrained model predicts arbitrary class probabilies that all look similar. This is because the initial weights of the Conv1d and Linear layers are small random values, so the output is only weakly sensitive to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.379],\n",
      "        [0.357],\n",
      "        [0.381],\n",
      "        [0.338],\n",
      "        [0.371]], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = torch.sigmoid(model(X))   # Turn activations into probabilities by feeding through sigmoid\n",
    "print(y_pred[:5])                  # Print the first few probabilities\n",
    "plot_named_tensors({'X': X, '$\\hat{y}$': y_pred, 'y': y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.3 &ndash; Train the ConvNet on your synthetic data\n",
    "\n",
    "Here you should train the ConvNet architecture from Exercise 1.2, much as you trained a PyTorch neural network in the Lab 8.\n",
    "\n",
    "First, **run the code cell below** to make a copy of the untrained model from Exercise 1.2. This will make it easy to re-run the training code cell multiple times, each time starting from an untrained model (rather than continuing to train the same model!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you'll need to define a loss function and implement a standard PyTorch training loop to train a copy of the untrained model. The PyTorch training loop for a ConvNet is identical to that of a neural network, so use Lab 8 as a guide.\n",
    "\n",
    "However, for this exercise use the following configuration:\n",
    "* *No mini-batches:* Unlike Lab 8, for this exercise do not use mini-batches. Instead feed the whole $X$ matrix through your model, like `model(X)` to get $N=75$ outputs. In other words, your training loop doesn't need an inner \"mini-batch\" loop.\n",
    "* *Loss function:* Use the \"binary cross entropy\" loss that directly accepts activations ('logits'). In PyTorch this is implemented by the **[torch.nn.BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss)** module. This is similar to the *CrossEntropyLoss* in Lab 8, except it's specialized to handle a single binary output.\n",
    "* *Optimizer:* This is the learning algorithm. Use the **[torch.optim.SGD](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD)** optimizer with learning rate $0.05$, momentum $0.9$, and weight decay (weight penalty) of $0.001$.\n",
    "\n",
    "Your training code should also print the current training loss for the first epoch and every 50 epochs after. The code for doing so is similar to in Lab 8. For example, if you computed your loss in variable `loss_value` then something like this would work:\n",
    "```python\n",
    "for epoch in range(1, num_epoch+1):\n",
    "    \n",
    "    ... your code to apply a step of gradient descent here\n",
    "    \n",
    "    if epoch == 1 or epoch % 50 == 0:\n",
    "        print(\"Epoch %d had training loss %.4f\" % (epoch, loss_value.item()))\n",
    "```\n",
    "where the `.item()` is a method that converts a scalar-valued PyTorch tensor into a standard Python value, like a *float*, so that it can be more easily formatted as part of the string.\n",
    "\n",
    "The output of your training loop should look exactly like this:\n",
    "```\n",
    "Epoch 1 had training loss 0.8562\n",
    "Epoch 50 had training loss 0.3519\n",
    "Epoch 100 had training loss 0.3281\n",
    "Epoch 150 had training loss 0.3212\n",
    "...\n",
    "```\n",
    "However, the loss should not go to zero for the current model architecture. You'll fix that shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d2/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 had training loss 0.6295537948608398\n",
      "Epoch 1 had training loss 0.6274372339248657\n",
      "Epoch 50 had training loss 0.28842294216156006\n",
      "Epoch 100 had training loss 0.09978275746107101\n",
      "Epoch 150 had training loss 0.03857172280550003\n",
      "Epoch 200 had training loss 0.021057013422250748\n",
      "Epoch 250 had training loss 0.014475857838988304\n",
      "Epoch 300 had training loss 0.011268023401498795\n",
      "Epoch 350 had training loss 0.009443060494959354\n",
      "Epoch 400 had training loss 0.008300545625388622\n",
      "Epoch 450 had training loss 0.007538574747741222\n"
     ]
    }
   ],
   "source": [
    "# Start from a copy of the untrained model, so that when we re-run this code\n",
    "# cell it always resets to the beginning.\n",
    "model = copy.deepcopy(untrained_model)\n",
    "\n",
    "# The number of times to evaluate the full training data (in this case, number of gradient steps)\n",
    "num_epoch = 500\n",
    "\n",
    "# Your code for defining loss, optimizer, and training loop here. Aim for 10-12 lines.\n",
    "loss = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=0.001)\n",
    "for epoch in range(num_epoch):\n",
    "    y_predict = model(X)\n",
    "    loss_value = loss(y_predict, y)\n",
    "    model.zero_grad()\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    if epoch == 1 or epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch} had training loss {loss_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot your trained model's predictions** in the code cell below. You should see that the prediction vector has an interesting pattern now (unlike the initial model), but that the predictions still do not quite match the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0.000],\n",
      "        [    0.983],\n",
      "        [    0.000],\n",
      "        [    0.993],\n",
      "        [    0.012]], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = torch.sigmoid(model(X))   # Turn activations into probabilities by feeding through sigmoid\n",
    "print(y_pred[:5])                  # Print the first few probabilities\n",
    "plot_named_tensors({'X': X, '$\\hat{y}$': y_pred, 'y': y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mismatch between predictions and targets is why the loss did not get close to zero, despite the fact that your ConvNet architecture can, in principle, learn a perfect classifier.\n",
    "\n",
    "If your model architecture was able to get the training loss to zero, the predictions would look more like this:\n",
    "\n",
    "![image](img/synthetic_predictions.png)\n",
    "\n",
    "<span style=\"color:red\">Currently,  your ConvNet training got stuck in a local minimum!</span>\n",
    "To give your ConvNet architecture a better chance at \"getting lucky\" and finding a path from random weights to useful weights, **try going back to Exercise 1.2 and increase the number of filters from 3 to 4**. You will then have to re-run all the code cells up to this point. With an extra filter, your ConvNet is now \"lucky\" enough to find at least one filter that detects a $0,1,0$ pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.4 Inspect the filters and weights of your ConvNet\n",
    "\n",
    "Here you should visualize both the filter weights and the linear (fully-connected) weights. Use the *plot_matrix_grid* function. If you managed to get your training loss to be small (close to zero) then at least one of your filters should look like this:\n",
    "\n",
    "![image](img/synthetic_filter_example.png)\n",
    "\n",
    "where red means positive and blue means negative. When this filter is convolved with a sequence containing pattern $\\ldots,0,1,0,\\ldots$ it will \"activate\" and have a large output at the position centered on the $1$. That activation then gets selected by the max pooling operation, and is then used to activate the output of the network, giving correct classification.\n",
    "\n",
    "**Write a few lines of plotting code** to visualize the weights of both layers. Do you see a correspondence between the filter that looks \"right\" and the positive weight(s) in the linear layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 5])\n",
      "torch.Size([4, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-524dc26d3db5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# The matrix of linear (fully-connected) weights that combine the filter responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Your plotting code here. Aim for 2-4 lines of code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msomething\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplot_matrix_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0m_arrays_for_stack_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'need at least one array to stack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0m_arrays_for_stack_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'need at least one array to stack'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \"\"\"\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = model.parameters()\n",
    "print(W1.shape)  # The tensor of 4 filters, each with 1 channel and kernel size 5\n",
    "print(W2.T.shape)  # The matrix of linear (fully-connected) weights that combine the filter responses\n",
    "# Your plotting code here. Aim for 2-4 lines of code.\n",
    "something = np.stack((W1, W2))\n",
    "plot_matrix_grid(W1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black\"></div>\n",
    "\n",
    "# 2. Digit classification with Convolutional Neural Networks\n",
    "\n",
    "Exercise 2.1&ndash;2.2 expand on Lab 8, this time asking you to train a convolutional neural network on the MNIST data set rather than a fully-connected neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1 Load and preprocess MNIST\n",
    "\n",
    "**Implement the *load_mnist_for_convnet* function** below. Rather than normalizing the features with scikit-learn, perform a simple normalization by scaling the pixel intensities from range $[0,255]$ down to $[0,1]$. This will be good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_mnist_for_convnet(filename):\n",
    "    \"\"\"\n",
    "    Loads the MNIST data from a Numpy NPZ file and returns two PyTorch tensors:\n",
    "    X: a float tensor with shape (N,1,28,28) where N is the number of images in the file\n",
    "    y: an int64 tensor with shape (N,) containing the class targets for the images.\n",
    "    The pixels values are scaled to be in range [0,1] where 0 is black and 1 is white.\n",
    "    \"\"\"\n",
    "    # Your code here. Aim for 7-10 lines.\n",
    "\n",
    "X_train, y_train = load_mnist_for_convnet(\"mnist_train.npz\")\n",
    "X_test,  y_test  = load_mnist_for_convnet(\"mnist_test.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(X_train, torch.FloatTensor), \"Features should be float32!\"\n",
    "assert isinstance(y_train, torch.LongTensor), \"Targets should be int64 or long!\"\n",
    "assert X_train.shape == (60000, 1, 28, 28), \"X_train has wrong shape\"\n",
    "assert y_train.shape == (60000,), \"y_train has wrong shape\"\n",
    "assert X_test.shape == (10000, 1, 28, 28), \"X_test has wrong shape\"\n",
    "assert y_test.shape == (10000,), \"y_test has wrong shape\"\n",
    "assert X_train.min() == 0 and X_train.max() == 1, \"Features don't seem to be scaled right!\"\n",
    "print(\"Looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "\n",
    "## Exericise 2.2 Train a Convolutional Neural Network on MNIST\n",
    "\n",
    "You are asked to train a 2D ConvNet to classify MNIST digits. You'll need the *Conv2d* and *MaxPool2d* modules as described in the [PyTorch modules documentation](https://pytorch.org/docs/stable/nn.html). Your architecture will be similar to Part 1 of this lab, except rather than pooling over the entire input signal you will pool only over small 2x2 regions, preserving more spatial information for the subsequent linear (fully-connected) layer to use in classification.\n",
    "\n",
    "**Write a few lines of code** to define a 2D ConvNet with the following feed-forward architecture:\n",
    "1. Convolution with 8 filters each of size 5x5 and enough padding to ensure that each of the resulting feature maps has spatial dimensions 28x28, just like the input.\n",
    "2. Rectified linear transformation (ReLU).\n",
    "3. Max pooling with kernel size 2x2 and a non-overlapping stride (i.e. stride same as kernel size).\n",
    "4. Flatten the spatial information\n",
    "5. Linear (fully-connected) layer to use all the outputs of the max pooling layer as features to make a prediction.\n",
    "\n",
    "(To predict class probabilities you'll have to apply a softmax function to the output, but that is normal in many deep learning frameworks because it makes training more numerically stable.)\n",
    "\n",
    "Hint: the trickiest part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # Ensure model weights initialized with same random numbers\n",
    "\n",
    "# Your code here. Aim for 8-11 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your model architecture** by trying to feed some inputs through. If an error is raised, then something is mis-configured in your network. The most likely error is that you did not correctly calculate the expected number of *in_features* for the linear layer, leading to a \"size mismatch\" in the tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(X_train[:5])  # Check model architecture by trying to feed inputs through it.\n",
    "assert y_pred.shape == (5, 10), \"Expected a batch of 5 images to produce output of shape (5, 10)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot your initial model's predictions** on a the first 30 training inputs by running the code cell below. Notice that once again all the predictions are pretty much the same, and are far from the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the model's predictions on 30 training cases through a row-wise softmax.\n",
    "y_pred = torch.softmax(model(X_train[:50]), dim=1)\n",
    "\n",
    "# Convert the first 30 training targets from index format {0,...,9} to a 1-hot format,\n",
    "# for easier side-by-side comparison with the 10-dimensional output prediction.\n",
    "y_true = torch.zeros((50, 10))\n",
    "y_true[torch.arange(50), y_train[:50]] = 1\n",
    "\n",
    "plot_named_tensors({'$\\hat{y}$': y_pred, '$y$': y_true})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspect your initial model's filters** by plotting them with the *plot_matrix_grid* function. They should look random and have small values roughly in range $[-0.1, 0.1]$ (Only plot the first layer filter weights, not the linear layer weights.) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your plotting code here. Aim for 1-2 lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train your model** with the following configuration:\n",
    "* Use the *CrossEntropyLoss* (since this is multi-class classification, not binary)\n",
    "* Use the exact same optimizer configuration you used for Part 1 of this lab (learning rate $0.05$, etc).\n",
    "* Use mini-batch training like you did in Lab 8, with a batch size of $100$.\n",
    "* Train for 5 epochs (5 passes over the full training set)\n",
    "\n",
    "First, define the loss function and optimizer in a separate code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_epoch = 5\n",
    "\n",
    "# Your code to define loss function and optimizer here. Aim for 2 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, write your training loop in a separate code cell below, so that you can re-run this code cell to \"continue\" training with the same optimizer object, if you want to train your model longer.\n",
    "\n",
    "I recommend you add a print statement to report progress like we did in Lab 8, such as:\n",
    "```\n",
    "Epoch 1 final minibatch had loss 0.2452\n",
    "Epoch 2 final minibatch had loss 0.2077\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your mini-batch training loop here. Aim for 9-12 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot your trained model's predictions**. The predictions should look different from before, and matching the targets better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 2-3 lines. Re-use the y_true matrix from the earlier code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only trained your ConvNet for 5 epochs total, you should be able to identify at least one training case that appears to be \"harder\" than the others, *i.e.* the model has a harder time giving a confident prediction. For example, here is an easy \"9\" and a hard \"9\" side by side:\n",
    "![image](img/easy_and_hard.png)\n",
    "\n",
    "**Plot an 'easy' training example and a 'hard' training example side-by-side** using the *plot_matrix_grid* function. This will require you to pull out two separate rows of *X_train* and turn them into a tensor with shape (2,28,28) so that the *plot_matrix_grid* knows what to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You code here. Aim for 1-4 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspect your trained model's filters** by plotting them with the *plot_matrix_grid* function. They should no longer be completely random and instead contain structures that look like little \"edge\" detectors, such as the \"diagonal line\" detector shown below\n",
    "![image](img/mnist_trained_filters.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here. Aim for 2-3 lines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
